{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from scipy import ndimage\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5087 rows and 3198 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(\".././\")\n",
    "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"data\", \"exoTest.csv\")).fillna(0)\n",
    "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"data\", \"exoTrain.csv\")).fillna(0)\n",
    "#print(train_data.head())\n",
    "#print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the labels\n",
    "# 0 indicates non-exoplanet star\n",
    "# 1 indicates exoplanet star\n",
    "label = {1: 0, 2: 1}\n",
    "\n",
    "train_data.LABEL = [label[item] for item in train_data.LABEL]\n",
    "test_data.LABEL = [label[item] for item in test_data.LABEL]\n",
    "\n",
    "#print(train_data.head())\n",
    "#print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is any feature with constant value through different stars\n",
    "# and drop the columns\n",
    "#print(train_data.nunique())\n",
    "counts = train_data.nunique()\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
    "# no constant column for these dataset\n",
    "print(\"Columns to be deleted\\n\")\n",
    "print(to_del)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Usage\n",
    "Reduce the memory usage of the dataframe by converting data types into smaller data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_memory(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "train_date = reduce_memory(train_data)\n",
    "test_data = reduce_memory(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the class distribution of stars to understand the ratio between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,10))\n",
    "colors = [\"0\", \"1\"]\n",
    "sns.color_palette(\"Set2\", 8)\n",
    "sns.countplot(x='LABEL', data=train_data)\n",
    "plt.title('Class Distributions \\n (0: Not Exoplanet, 1: Exoplanet)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "import random\n",
    "rcParams['figure.figsize'] = 13, 8\n",
    "plt.title('Distribution of flux values', fontsize=10)\n",
    "plt.xlabel('Flux values')\n",
    "plt.ylabel('Flux intensity')\n",
    "rand_stars = list()\n",
    "random.seed(27)\n",
    "for i in range(4):\n",
    "    rand_stars.append(random.randint(0, train_data.shape[0]))\n",
    "plt.plot(train_data.iloc[rand_stars[0],])\n",
    "plt.plot(train_data.iloc[rand_stars[1],])\n",
    "plt.plot(train_data.iloc[rand_stars[2],])\n",
    "plt.plot(train_data.iloc[rand_stars[3],])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization\n",
    "Considering above graph of flux values, some of the flux values has a wider range than others which suppresses the effect of other features. Therefore our dataset needs to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 25, 4\n",
    "fig, axs = plt.subplots(1,4)\n",
    "fig.suptitle(\"Gaussian Histograms - Absense of Exoplanets\")\n",
    "axs[0].hist(train_data.iloc[44,:], bins=200)\n",
    "axs[0].set_xlabel(\"Flux values\")\n",
    "axs[1].hist(train_data.iloc[47,:], bins=200)\n",
    "axs[1].set_xlabel(\"Flux values\")\n",
    "axs[2].hist(train_data.iloc[60,:], bins=200)\n",
    "axs[2].set_xlabel(\"Flux values\")\n",
    "axs[3].hist(train_data.iloc[299,:], bins=200)\n",
    "axs[3].set_xlabel(\"Flux values\")\n",
    "\n",
    "\n",
    "rcParams['figure.figsize'] = 20, 4\n",
    "fig, axs = plt.subplots(1,4)\n",
    "fig.suptitle(\"Gaussian Histograms - Presense of Exoplanets\")\n",
    "axs[0].hist(train_data.iloc[10,:], bins=200)\n",
    "axs[0].set_xlabel(\"Flux values\")\n",
    "axs[1].hist(train_data.iloc[19,:], bins=200)\n",
    "axs[1].set_xlabel(\"Flux values\")\n",
    "axs[2].hist(train_data.iloc[24,:], bins=200)\n",
    "axs[2].set_xlabel(\"Flux values\")\n",
    "axs[3].hist(train_data.iloc[30,:], bins=200)\n",
    "axs[3].set_xlabel(\"Flux values\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the presense of exoplanets, there could be seen fluctuations which is resulted from the exoplanet that circles around its star. *** DÃ¼zelt ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.drop([\"LABEL\"],axis=1)\n",
    "y_train = train_data[\"LABEL\"]\n",
    "x_test = test_data.drop([\"LABEL\"],axis=1)\n",
    "y_test = test_data[\"LABEL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize each sample independently (default axis is 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Flux values before normalization\")\n",
    "print(x_train.iloc[0:5,])\n",
    "x_train = normalized = normalize(x_train) \n",
    "x_test = normalize(x_test)\n",
    "print(\"Flux values after normalization\")\n",
    "print(x_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = filtered = ndimage.filters.gaussian_filter(x_train, sigma=10)\n",
    "x_test = ndimage.filters.gaussian_filter(x_test, sigma=10)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, data is standardized since our attribution distribution is Gaussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "x_train = scaled = std_scaler.fit_transform(x_train)\n",
    "x_test = std_scaler.fit_transform(x_test)\n",
    "print(\"Data after standardization: \")\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimentionality Reduction\n",
    "\n",
    "There are **3198** features particularly for this dataset. To deal with the excessive number of features, our model should be provided with the subset of these features which would provide nearly the same amount of information with a lesser number of features. This is where the **Dimentionality Reduction** comes into play.\n",
    "\n",
    "First, we need to determine the number of features that would be enough to classify our data using **PCA** which is the *Principal Component Analysis*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # import PCA\n",
    "pca = PCA()\n",
    "x_train = pca.fit_transform(x_train)\n",
    "\n",
    "\"\"\"\n",
    "Do not fit the test data since including the test dataset\n",
    "in the transform computation will allow information\n",
    "to flow from the test data to the train data which\n",
    "would distort the accuracy we will obtain from models\n",
    "\n",
    "\"\"\"\n",
    "x_test = pca.transform(x_test) \n",
    "total=sum(pca.explained_variance_)\n",
    "k=0\n",
    "current_variance=0\n",
    "\"\"\"\n",
    "After tranforming the data, PCA gives us the explained\n",
    "variance of each feature. We should obtain a cumulative\n",
    "sum of features with total variance ratio of 90%.\n",
    "\"\"\"\n",
    "while current_variance/total < 0.90:\n",
    "    current_variance += pca.explained_variance_[k]\n",
    "    k=k+1\n",
    "\n",
    "print(\"Number of features after PCA: \", k)\n",
    "N_FEATURES = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "#Apply PCA with n_componenets\n",
    "pca = PCA(n_components=N_FEATURES)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Exoplanet Dataset Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on how effective SVM is in high dimensional spaces, dataset was trained and tested with different configurations of SVM: *C value and kernel*. Even though the model did give\n",
    "high accuracies with each possible pairs, the best parameters were **C = 1 and kernel = rbf**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\"\"\"\n",
    "Ignore ill-defined scores warning since the model\n",
    "did not predict any data as false-positive or false-negative\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100, 1000], 'kernel': ['linear', 'rbf']} \n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, n_jobs=-1) \n",
    "   \n",
    "# fit the model with train data\n",
    "grid.fit(x_train, y_train) \n",
    " \n",
    "# print the best parameters after tuning \n",
    "print(\"Best parameters: \", grid.best_params_) \n",
    "grid_predictions = grid.predict(x_test) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(y_test, grid_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reason that most of the results lie between a narrow range, effect of the slight changes cannot be reflected to the graphps. As a result, MidpointNormalize class is defined to redefine the midpoints (focus points) of the ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return ma.masked_array(np.interp(value, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,15))\n",
    "plt.subplot(221)\n",
    "sns.heatmap(confusion_matrix(y_test, grid_predictions), annot=True, cmap=\"viridis\", fmt = \"d\", linecolor=\"k\", linewidths=3, norm=MidpointNormalize(vmin=0.2, midpoint=0.95))\n",
    "plt.title(\"CONFUSION MATRIX\",fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(6, 2)\n",
    "\n",
    "\"\"\" \n",
    "Draw heatmap of the validation accuracy as a function of C and kernel function.\n",
    "The score are encoded as colors with the hot colormap which varies from dark\n",
    "red to bright yellow. As the most all of the scores are located in the\n",
    "0.95 to 0.99 range, a custom normalizer is used to set the mid-point to 0.95 so\n",
    "as to make it easier to visualize the small variations of score values in the\n",
    "interesting range while not brutally collapsing all the low score values to\n",
    "the same color.\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot, norm=MidpointNormalize(vmin=0.2, midpoint=0.95))\n",
    "plt.xlabel('Kernel')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(param_grid['kernel'])), param_grid['kernel'], rotation=45)\n",
    "plt.yticks(np.arange(len(param_grid[\"C\"])), param_grid[\"C\"])\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the heatmap above which displays accuracies of the model with different configurations, we can deduce that exoplanets dataset that is used in this model is linearly seperable with couple of points in the gutter since low values of C also gave high accuracies."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21d20391fde7c5334170d4e69ce1e6049db42d10324e9a5c1b1b1b7bb352ac00"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('exoplanet': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
